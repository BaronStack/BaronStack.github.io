{"title":"Python爬虫 爬取批量图片","slug":"Python爬虫-爬取批量图片","date":"2018-01-26T10:40:37.000Z","updated":"2018-02-22T12:54:00.000Z","comments":true,"path":"api/articles/Python爬虫-爬取批量图片.json","photos":[],"link":"","excerpt":"本部分的主要代码在<br>爬取批量图片到本地如何用python 爬取图片网站的图片这里的图片是按页显示的，所以它爬取的是每一页的图片获取网页链接利用正则提取以页为单位的html内容获取每一页中图片的存储链接利用循环下载保存","covers":["/images/python/scrap_images.png","/images/python/scrap_images_d.png"],"content":"<p>本部分的主要代码在<br><a href=\"https://github.com/BaronStack/Python-Reptitle/tree/master/reptitle_trying\" target=\"_blank\" rel=\"noopener\">爬取批量图片到本地</a></p>\n<h4 id=\"如何用python-爬取图片网站的图片\"><a href=\"#如何用python-爬取图片网站的图片\" class=\"headerlink\" title=\"如何用python 爬取图片网站的图片\"></a>如何用python 爬取图片网站的图片</h4><blockquote>\n<p>这里的图片是按页显示的，所以它爬取的是每一页的图片</p>\n</blockquote>\n<ul>\n<li>获取网页链接</li>\n<li>利用正则提取以页为单位的html内容</li>\n<li>获取每一页中图片的存储链接</li>\n<li>利用循环下载保存<a id=\"more\"></a></li>\n</ul>\n<p>导入的包如下：<br><figure class=\"highlight capnproto\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup <span class=\"comment\"># 方便提取标签中的内容</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> requests <span class=\"comment\"># 向网页发送请求的包，获取网页内容</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> re <span class=\"comment\"># 正则表达式的包</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> urllib.request <span class=\"keyword\">import</span> urlopen <span class=\"comment\"># 也是用来获取网页内容</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os <span class=\"comment\"># 文件目录操作</span></span><br></pre></td></tr></table></figure></p>\n<h5 id=\"获取网页链接，利用正则提取以页为单位的html链接\"><a href=\"#获取网页链接，利用正则提取以页为单位的html链接\" class=\"headerlink\" title=\"获取网页链接，利用正则提取以页为单位的html链接\"></a>获取网页链接，利用正则提取以页为单位的html链接</h5><figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 这里也可以用语句 html = requests.get(\"http://www.ivsky.com/tupian/xuejing_t36/index_2.html\").text来获取网页内容</span></span><br><span class=\"line\"><span class=\"attr\">html</span> = urlopen(<span class=\"string\">\"http://www.ivsky.com/tupian/xuejing_t36/index_2.html\"</span>).read().decode(<span class=\"string\">'utf-8'</span>) </span><br><span class=\"line\"><span class=\"comment\">#利用BeautifulSoup对html进行解析，从而可以获取网页的节点属性</span></span><br><span class=\"line\"><span class=\"attr\">soup</span> = BeautifulSoup(html, features=<span class=\"string\">'lxml'</span>)</span><br><span class=\"line\"><span class=\"comment\">#找到以'panelist'为class的div标签的内容（这里存储的不同页面的html链接）</span></span><br><span class=\"line\"><span class=\"attr\">img_links</span> = soup.find_all(<span class=\"string\">\"div\"</span>, &#123;<span class=\"string\">\"class\"</span>: <span class=\"string\">'pagelist'</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#利用正则表达式，将标签中的链接取出来，形成列表。这里可以打开链接</span></span><br><span class=\"line\"><span class=\"attr\">course_links</span> = soup.find_all(<span class=\"string\">'a'</span>, &#123;<span class=\"string\">'href'</span>: re.compile(<span class=\"string\">'/tupian/.*index_.\\.html$'</span>)&#125;)</span><br><span class=\"line\"><span class=\"comment\">#这里的链接是没有http://跳转的，所以给其中每个链接都加上这样的跳转</span></span><br><span class=\"line\"><span class=\"attr\">resultURL</span> = [<span class=\"string\">\"%s%s\"</span> %(<span class=\"string\">'http://www.ivsky.com'</span>,links[<span class=\"string\">'href'</span>]) for links in course_links]</span><br></pre></td></tr></table></figure>\n<h5 id=\"获取每一页中图片的存储链接\"><a href=\"#获取每一页中图片的存储链接\" class=\"headerlink\" title=\"获取每一页中图片的存储链接\"></a>获取每一页中图片的存储链接</h5><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#取得当前页面的URL</span><br><span class=\"line\"><span class=\"selector-tag\">html</span> = requests.get(URL).text</span><br><span class=\"line\">#解析</span><br><span class=\"line\">soup = BeautifulSoup(<span class=\"selector-tag\">html</span>, <span class=\"string\">'lxml'</span>)</span><br><span class=\"line\">#找到以il_img为class的div标签</span><br><span class=\"line\">img_ul = soup.find_all(<span class=\"string\">'div'</span>, &#123;<span class=\"string\">'class'</span>: <span class=\"string\">\"il_img\"</span>&#125;)</span><br><span class=\"line\">#查看找到的区域数量</span><br><span class=\"line\"><span class=\"function\"><span class=\"title\">print</span><span class=\"params\">(len(img_ul)</span></span>)</span><br></pre></td></tr></table></figure>\n<p>这里在利用浏览器的检查元素进行查找，在找的过程中尽可能将当前页面图片进行归类分析，找出其共有的标签属性，方便将所有的图片进行爬取</p>\n<h5 id=\"利用循环下载保存\"><a href=\"#利用循环下载保存\" class=\"headerlink\" title=\"利用循环下载保存\"></a>利用循环下载保存</h5><figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">os.makedirs(<span class=\"string\">'./img/'</span>, exist_ok=True)<span class=\"comment\">#创建保存的文件夹及存储目录，可以自行更改</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> ul <span class=\"keyword\">in</span> img_ul:</span><br><span class=\"line\">        imgs = ul.find_all(<span class=\"string\">'img'</span>)<span class=\"comment\">#找到 img标签</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> img <span class=\"keyword\">in</span> imgs:</span><br><span class=\"line\">            url = img[<span class=\"string\">'src'</span>] <span class=\"comment\">#找到src标签，因为其后跟着图片链接</span></span><br><span class=\"line\">            r = requests.<span class=\"built_in\">get</span>(url, stream=True)</span><br><span class=\"line\">            image_name = url.<span class=\"built_in\">split</span>(<span class=\"string\">'/'</span>)[<span class=\"number\">-1</span>] <span class=\"comment\">#图片以最后一个'／'右边的内容明名，也可以自行命名</span></span><br><span class=\"line\">            <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">'./img/%s'</span> % image_name, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> chunk <span class=\"keyword\">in</span> r.iter_content(chunk_size=<span class=\"number\">128</span>): <span class=\"comment\">#将图片分块以数据流的方式直接写入到本地</span></span><br><span class=\"line\">                    f.<span class=\"built_in\">write</span>(chunk)</span><br><span class=\"line\">            print(<span class=\"string\">'Saved %s'</span> % image_name) <span class=\"comment\">#打印保存的进度</span></span><br></pre></td></tr></table></figure>\n<p>最后看看我们的结果：</p>\n<p><img src=\"/images/python/scrap_images.png\" alt=\"运行结果\"></p>\n<p><img src=\"/images/python/scrap_images_d.png\" alt=\"保存结果\"></p>\n<p>所以<strong>妈妈再也不用担心我没有素材啦</strong></p>\n","categories":[{"name":"python","slug":"python","count":3,"path":"api/categories/python.json"}],"tags":[{"name":"python","slug":"python","count":5,"path":"api/tags/python.json"},{"name":"爬虫","slug":"爬虫","count":1,"path":"api/tags/爬虫.json"}]}